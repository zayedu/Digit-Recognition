{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4216afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1830c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "  root=\"data\",\n",
    "  train=True,\n",
    "  transform=ToTensor(),\n",
    "  download=True\n",
    "  \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "  root=\"data\",\n",
    "  train=False,\n",
    "  transform=ToTensor(),\n",
    "  download=True\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c55cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders={\n",
    "  'train': DataLoader(train_data, \n",
    "                     batch_size=100,\n",
    "                     shuffle=True,\n",
    "                     num_workers=1),\n",
    "  'test': DataLoader(test_data, \n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    num_workers=1),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3532fc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x155df3a10>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x155df3a90>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee36cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        x= F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) \n",
    "        \n",
    "        x = x.view(-1, 20*4*4)  \n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.softmax(x)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee460eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model= CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 20 == 0:\n",
    "      print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "      \n",
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in loaders['test']:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      test_loss += loss_fn(output, target).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      \n",
    "  test_loss /= len(loaders['test'].dataset)\n",
    "  print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d96ad3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/kp8473s15qj359p037xgnv8m0000gn/T/ipykernel_91520/2429167214.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303497\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.292529\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.189135\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.065634\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.948128\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.896376\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.854716\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 1.719756\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.848924\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.728699\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.823552\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 1.758040\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.709952\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 1.643866\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.615521\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.691000\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.630526\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 1.676932\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.661536\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 1.641044\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.679319\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 1.611496\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 1.587609\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 1.617970\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.613210\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.653646\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 1.576427\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 1.611593\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.614622\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 1.604394\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9283/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.596947\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 1.624053\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 1.595596\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 1.555556\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1.637831\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.539356\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 1.598010\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 1.638354\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.603835\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 1.587520\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.573020\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 1.650838\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 1.599493\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 1.547636\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 1.616492\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.577040\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.558029\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 1.623508\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 1.652269\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 1.591787\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.553486\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 1.526694\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 1.548821\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 1.597291\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.552897\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.568762\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 1.586358\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 1.641553\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 1.565705\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 1.557311\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 9551/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.574716\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 1.540282\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 1.617598\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 1.576356\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 1.562850\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.529845\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 1.573017\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 1.550939\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.599618\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 1.523522\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.535374\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 1.536796\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 1.509312\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 1.573182\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 1.556943\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.574811\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.550245\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 1.545019\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 1.559008\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 1.539849\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.567842\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 1.578198\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 1.545824\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 1.562348\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.557622\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.505852\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 1.553789\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 1.537190\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 1.590640\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 1.535131\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9601/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.569170\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 1.543120\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 1.570082\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 1.577911\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 1.561961\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.570013\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 1.504239\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 1.532707\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 1.566834\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 1.625561\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.555489\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 1.569708\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 1.533184\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 1.564125\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 1.573759\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.573107\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.542402\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 1.561461\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 1.554749\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 1.568224\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.509962\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 1.573144\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 1.538795\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 1.537098\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.559004\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.565020\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 1.512249\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 1.537857\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 1.547879\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 1.498296\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9648/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.513762\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 1.532299\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 1.479194\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 1.516569\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 1.534243\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.582564\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 1.536874\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 1.526197\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 1.552643\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 1.524697\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.601793\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 1.542465\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 1.568160\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 1.551183\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 1.563925\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.542549\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.507891\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 1.493182\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 1.531615\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 1.495706\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.577810\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 1.565856\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 1.540154\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 1.571677\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 1.520736\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.535479\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 1.557091\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 1.536930\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 1.511628\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 1.514932\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.563332\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 1.495408\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 1.558702\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 1.520628\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 1.543777\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.560016\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 1.567425\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 1.533165\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 1.526944\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 1.554678\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.574253\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 1.560712\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 1.507904\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 1.572094\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 1.527856\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.527139\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.527800\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 1.497813\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 1.558200\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 1.554500\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.548616\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 1.486453\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 1.506004\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 1.496964\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 1.553200\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.508790\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 1.543808\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 1.510168\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 1.519007\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 1.530823\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9703/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.588881\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 1.533274\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 1.533787\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 1.538697\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 1.518559\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.539186\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 1.526132\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 1.514174\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 1.531970\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 1.537499\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.512454\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 1.595658\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 1.542950\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 1.521593\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 1.515734\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.508717\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.551489\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 1.516835\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 1.513140\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 1.547642\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.521745\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 1.521761\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 1.530440\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 1.494871\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 1.506410\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.540929\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 1.551655\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 1.526691\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 1.559143\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 1.511759\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.520630\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 1.508619\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 1.526467\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 1.524732\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 1.500315\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.570317\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 1.527471\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 1.535421\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 1.494948\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 1.578457\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.503932\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 1.500699\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 1.580399\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 1.481806\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 1.559037\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.554519\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.503535\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 1.541590\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 1.519763\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 1.518108\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.508245\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 1.519449\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 1.539172\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 1.511557\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 1.519689\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.519994\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 1.493291\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 1.501136\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 1.490355\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 1.527355\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.530605\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 1.512447\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 1.541138\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 1.573251\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 1.562277\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.502578\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 1.539583\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 1.525680\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 1.535614\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 1.541615\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.503344\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 1.519354\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 1.508489\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 1.493294\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 1.516069\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.520060\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.499457\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 1.514387\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 1.511817\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 1.556351\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.511774\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 1.554154\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 1.538494\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 1.566698\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 1.514829\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.520189\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 1.528619\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 1.514171\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 1.497564\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 1.546463\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.551114\n",
      "Train Epoch: 10 [2000/60000 (3%)]\tLoss: 1.513524\n",
      "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 1.521185\n",
      "Train Epoch: 10 [6000/60000 (10%)]\tLoss: 1.525942\n",
      "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 1.510502\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 1.562036\n",
      "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 1.498227\n",
      "Train Epoch: 10 [14000/60000 (23%)]\tLoss: 1.529586\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 1.565438\n",
      "Train Epoch: 10 [18000/60000 (30%)]\tLoss: 1.540107\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.514332\n",
      "Train Epoch: 10 [22000/60000 (37%)]\tLoss: 1.556749\n",
      "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 1.533796\n",
      "Train Epoch: 10 [26000/60000 (43%)]\tLoss: 1.520930\n",
      "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 1.517544\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.515187\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.554912\n",
      "Train Epoch: 10 [34000/60000 (57%)]\tLoss: 1.509418\n",
      "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 1.482593\n",
      "Train Epoch: 10 [38000/60000 (63%)]\tLoss: 1.494132\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.545553\n",
      "Train Epoch: 10 [42000/60000 (70%)]\tLoss: 1.511126\n",
      "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 1.498559\n",
      "Train Epoch: 10 [46000/60000 (77%)]\tLoss: 1.533499\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 1.509479\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.501029\n",
      "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 1.543039\n",
      "Train Epoch: 10 [54000/60000 (90%)]\tLoss: 1.508661\n",
      "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 1.512457\n",
      "Train Epoch: 10 [58000/60000 (97%)]\tLoss: 1.505157\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9752/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5878f71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9977207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/kp8473s15qj359p037xgnv8m0000gn/T/ipykernel_91520/2429167214.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "data,target=test_data[10]\n",
    "\n",
    "data=data.unsqueeze(0).to(device)\n",
    "output=model(data)\n",
    "\n",
    "predi=output.argmax(dim=1, keepdim=True).item()\n",
    "print(f'Prediction: {predi}')\n",
    "\n",
    "image=data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505544d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
